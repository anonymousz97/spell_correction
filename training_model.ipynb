{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be4afb87-b291-4537-a718-2b314728d1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f1557e4c3d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gen_err import SynthesizeData\n",
    "from model import SpellCorrectionModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch import Tensor\n",
    "import traceback\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "import ast\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16b446b9-fdc2-4bd0-a540-7aa2f118c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('merge_word_vocab_13126.txt','r') as f:\n",
    "    word_tokenizer = f.read().split('\\n')\n",
    "word_tokenizer.insert(0,\"<unk>\")\n",
    "word_tokenizer.insert(0,\"<pad>\")\n",
    "# word_tokenizer.insert(0,\"<end>\")\n",
    "# word_tokenizer.insert(0,\"<start>\")\n",
    "\n",
    "map_word = {j:i for i,j in enumerate(word_tokenizer)}\n",
    "reverse_map_word = {map_word[i]:i for i in map_word.keys()}\n",
    "# map_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dfb1659-b6d3-4a38-abea-7e62e04875be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('char_vocab.txt','r') as f:\n",
    "    char_tokenizer = f.read().split('\\n')\n",
    "char_tokenizer.insert(0,\"<unk>\")\n",
    "# char_tokenizer.insert(0,\"<end>\")\n",
    "# char_tokenizer.insert(0,\"<start>\")\n",
    "char_tokenizer.insert(0,\"<pad>\")\n",
    "\n",
    "map_char = {j:i for i,j in enumerate(char_tokenizer)}\n",
    "reverse_map_char = {map_char[i]:i for i in map_char.keys()}\n",
    "# map_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b83376c-ff30-43f0-80d4-1fff2aa46f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_batch(sample):\n",
    "    try:\n",
    "        max_word_length = max([len(x.split(' ')) for x in sample])\n",
    "        max_char_length = max([max([len(word) for word in sentence.split(' ')]) for sentence in sample])\n",
    "    except:\n",
    "        return \"\", \"\", 0\n",
    "    # #print(max_word_length, max_char_length)\n",
    "    res = []\n",
    "    res_char = []\n",
    "    for i in sample:\n",
    "        out = []\n",
    "        out_char = []\n",
    "        for j in i.split(' '):\n",
    "            out.append(map_word[j.lower()] if j.lower() in map_word.keys() else map_word['<unk>'])\n",
    "            r = []\n",
    "            for g in j:\n",
    "                r.append(map_char[g] if g in map_char.keys() else map_char['<unk>'])\n",
    "            while len(r) < max_char_length:\n",
    "                r.append(0)\n",
    "            out_char.append(r)\n",
    "        while len(out) < max_word_length:\n",
    "            out.append(0)\n",
    "            out_char.append([0] * max_char_length)\n",
    "            \n",
    "        res.append(out)\n",
    "        res_char.append(out_char)\n",
    "    return res, res_char , max_word_length\n",
    "\n",
    "# mapping_batch(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcb69f3b-a7de-4678-9143-d72a8b755996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[3581, 3364, 965, 729, 2622]],\n",
       " [[[21, 126, 103, 174, 0],\n",
       "   [128, 0, 0, 0, 0],\n",
       "   [136, 148, 155, 0, 0],\n",
       "   [155, 126, 96, 159, 0],\n",
       "   [136, 125, 160, 148, 127]]],\n",
       " 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = [\"Chây ì nộp phạt nguội\"]\n",
    "mapping_batch(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f8cb96a-4726-49a5-b6ff-9208cceb180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Viet74K.txt','r') as f:\n",
    "#     data = f.read().split()\n",
    "# data = [x.lower().split(' ')[0] for x in data]\n",
    "# data = list(set(data))\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db220c66-7dd4-4bf2-a912-e14a7be91fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vietnamese.txt','w') as f:\n",
    "#     for i in data:\n",
    "#         f.write(i)\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdfe8a81-83f0-48fd-a160-aaa14f35d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_spell = SynthesizeData(\"vietnamese.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e377759f-f561-44ae-a45e-797ecf2b21e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đây 0\n",
      "cũng 0\n",
      "là 0\n",
      "trận 0\n",
      "đấu 0\n",
      "chính 0\n",
      "thức 0\n",
      "đầu 0\n",
      "tiên 0\n",
      "của 0\n",
      "huấn 0\n",
      "luyện 0\n",
      "viên 0\n",
      "Kiatisak 0\n",
      "sau 0\n",
      "khi 0\n",
      "đảm 0\n",
      "nhận 0\n",
      "cuwơng 1\n",
      "vị 0\n",
      "huấn 0\n",
      "luyện 0\n",
      "viên 0\n",
      "trưởng 0\n",
      "của 0\n",
      "Công 0\n",
      "an 0\n",
      "Hxà 1\n",
      "Nội 0\n",
      ". 0\n"
     ]
    }
   ],
   "source": [
    "res = gen_spell.add_noise(\"Đây cũng là trận đấu chính thức đầu tiên của huấn luyện viên Kiatisak sau khi đảm nhận cương vị huấn luyện viên trưởng của Công an Hà Nội. \", 0.2)\n",
    "for i,j in zip(res[0].split(' '), res[1]):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1305de10-4220-40fd-81f5-2d4bdb03b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "aàáảãạ\n",
    "ăằắẳẵặ\n",
    "âầấẩẫậ\n",
    "bc\n",
    "dđ\n",
    "eèéẻẽẹ\n",
    "êềếểễệ\n",
    "gh\n",
    "iìíỉĩị\n",
    "lkmn\n",
    "oòóỏõọ\n",
    "ôồốổỗộ\n",
    "ơờớởỡợ\n",
    "pqrst\n",
    "uùúủũụ\n",
    "ưừứửữự\n",
    "vx\n",
    "yỳýỷỹỵ\n",
    "\"\"\"\n",
    "all_char = []\n",
    "for i in s:\n",
    "    if i == '\\n':\n",
    "        continue\n",
    "    all_char.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38de3120-7542-4e37-996a-36ad6a27a27a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'á',\n",
       " 'ả',\n",
       " 'ã',\n",
       " 'ạ',\n",
       " 'ă',\n",
       " 'ằ',\n",
       " 'ắ',\n",
       " 'ẳ',\n",
       " 'ẵ',\n",
       " 'ặ',\n",
       " 'â',\n",
       " 'ầ',\n",
       " 'ấ',\n",
       " 'ẩ',\n",
       " 'ẫ',\n",
       " 'ậ',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'đ',\n",
       " 'e',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ẻ',\n",
       " 'ẽ',\n",
       " 'ẹ',\n",
       " 'ê',\n",
       " 'ề',\n",
       " 'ế',\n",
       " 'ể',\n",
       " 'ễ',\n",
       " 'ệ',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'ì',\n",
       " 'í',\n",
       " 'ỉ',\n",
       " 'ĩ',\n",
       " 'ị',\n",
       " 'l',\n",
       " 'k',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'ò',\n",
       " 'ó',\n",
       " 'ỏ',\n",
       " 'õ',\n",
       " 'ọ',\n",
       " 'ô',\n",
       " 'ồ',\n",
       " 'ố',\n",
       " 'ổ',\n",
       " 'ỗ',\n",
       " 'ộ',\n",
       " 'ơ',\n",
       " 'ờ',\n",
       " 'ớ',\n",
       " 'ở',\n",
       " 'ỡ',\n",
       " 'ợ',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'ù',\n",
       " 'ú',\n",
       " 'ủ',\n",
       " 'ũ',\n",
       " 'ụ',\n",
       " 'ư',\n",
       " 'ừ',\n",
       " 'ứ',\n",
       " 'ử',\n",
       " 'ữ',\n",
       " 'ự',\n",
       " 'v',\n",
       " 'x',\n",
       " 'y',\n",
       " 'ỳ',\n",
       " 'ý',\n",
       " 'ỷ',\n",
       " 'ỹ',\n",
       " 'ỵ']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "442a6974-6289-446d-b91b-196cc190405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def split_punc_to_word2(sentence):\n",
    "    separated_sentence = sentence.replace(\"'\",'\"')\n",
    "    separated_sentence = re.sub(r\"([\\w/'+$\\s-]+|[^\\w/'+$\\s-]+)\\s*\", r\"\\1 \", sentence)\n",
    "    # print(separated_sentence)\n",
    "    separated_sentence = \" \".join(separated_sentence.split())\n",
    "    return separated_sentence\n",
    "\n",
    "\n",
    "def split_punc_to_word(sentence):\n",
    "    tmp = sentence.split(' ')\n",
    "    res = []\n",
    "    for i in tmp:\n",
    "        r, l = -1, len(i) + 1\n",
    "        for idx,char in enumerate(i):\n",
    "            if char.isalpha() and char.lower() in all_char:\n",
    "                if r == -1:\n",
    "                    r = idx\n",
    "                l = idx\n",
    "        if l == len(i) + 1:\n",
    "            l = len(i)\n",
    "        res.append(\" \".join((i[:r] + \" \" + i[r:l+1] + \" \" + i[l+1:]).split()))\n",
    "    out = \" \".join(x for x in res)\n",
    "    return split_punc_to_word2(out)\n",
    "                    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f7c9f4e-0164-483a-9e7b-d52a696d96e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hàng chục ngàn ), phuong tiện bỵ ghi hình vi phạm luật gizo thông ở P . HCM , bị ' bêu tên ' nhưng chủ vẫn khôn chịu nộp phạt .\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Hàng chục ngàn), phuong tiện bỵ ghi hình vi phạm luật gizo thông ở P . HCM , bị 'bêu tên ' nhưng chủ vẫn khôn chịu nộp phạt .\"\n",
    "split_punc_to_word(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f4cc3ac-209b-4e61-8e40-6c92939a20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gen training data\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "# import pandas as pd\n",
    "# cnt = 0\n",
    "# lst_gt = []\n",
    "# lst_gen = []\n",
    "# spell_label = []\n",
    "# cap_label = []\n",
    "# with open('corpus-full.txt', 'r') as file:\n",
    "#     for line in tqdm(file, total=cnt):\n",
    "#         cnt += 1\n",
    "#         s = []\n",
    "#         line = split_punc_to_word(line)\n",
    "#         # print(line)\n",
    "#         res = gen_spell.add_noise(line)\n",
    "#         # print(res)\n",
    "#         lst_gt.append(line)\n",
    "#         lst_gen.append(res[0])\n",
    "#         label_cap = [1 if x[0].isupper() else 0 for x in line.split(' ')]\n",
    "#         label_spell = [0 if x != 1 else 1 for x in res[1]]\n",
    "#         spell_label.append(label_spell)\n",
    "#         cap_label.append(label_cap)\n",
    "#         if cnt == 1000000:\n",
    "#             break\n",
    "\n",
    "# df = pd.DataFrame({\"text\":lst_gt,'generate':lst_gen,\"spell_label\":spell_label,\"cap_label\":cap_label})\n",
    "# # print(df)\n",
    "# df.to_csv('train_format.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5203979-e943-4707-b91e-858a2af42e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom loss function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "# def custom_focal_loss(output, target):\n",
    "#     alpha = 0.25  # balancing parameter\n",
    "#     gamma = 2.0   # focusing parameter\n",
    "#     bce_loss = bce_l(output, target)\n",
    "#     pt = torch.exp(-bce_loss)\n",
    "#     focal_loss = alpha * (1 - pt) ** gamma * bce_loss\n",
    "#     return focal_loss\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weight = [1,1,1]):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, output_correction, output_spell, output_upper, target, total_length):\n",
    "        for k in target.keys():\n",
    "            if k == 'correction':\n",
    "                target[k] = target[k].to(device)\n",
    "            else:\n",
    "                target[k] = target[k].to(device).float()\n",
    "        # print(target_correction.shape, output_correction.shape, output_spell.shape, target_spell.shape)\n",
    "        # Calculate the loss for is_correct prediction\n",
    "        # bce_loss_upper = nn.BCEWithLogitsLoss(reduction='mean', pos_weight=(target['upper']*3).view(-1, 1)).to(device)\n",
    "        # bce_loss_spell = nn.BCEWithLogitsLoss(reduction='mean', pos_weight=(target['spell']*3).view(-1, 1)).to(device)\n",
    "        output_spell = output_spell.float()\n",
    "        # print(output_spell)\n",
    "        output_upper = output_upper.float()\n",
    "        # print(output_spell.shape, output_upper.shape)\n",
    "        # print(target['spell'].shape, target['upper'].shape)\n",
    "        # Calculate the loss for correction prediction\n",
    "        mask = (target['spell'] != 0.0).float()\n",
    "\n",
    "        # print(\"Output spell \",output_spell)\n",
    "        \n",
    "        # Calculate cross-entropy loss with the mask\n",
    "\n",
    "        # print(output_correction.shape, mask.shape)\n",
    "\n",
    "        output_correction = torch.permute(output_correction, (0,2,1))\n",
    "        loss = F.cross_entropy(output_correction, target['correction'], reduction='none')  # 'none' to prevent reduction\n",
    "        # print(\"loss \",loss.shape)\n",
    "        # Apply the mask to ignore certain positions\n",
    "        masked_loss = loss * mask\n",
    "        \n",
    "        # Calculate the mean loss ignoring certain positions\n",
    "        correction_loss = torch.sum(masked_loss) / (torch.sum(mask)+ 1e-6)\n",
    "        # print(\"corr \",correction_loss)\n",
    "        # print(\"corr loss after mask \",correction_loss)\n",
    "        is_correct_loss = bce_l(output_spell.view(-1, 1), target['spell'].view(-1, 1)) / total_length\n",
    "        # print(\"is_correct_loss loss \",is_correct_loss)\n",
    "        is_upper_loss = bce_l(output_upper.view(-1, 1), target['upper'].view(-1, 1)) / total_length\n",
    "        # print(\"is_upper_loss loss \",is_upper_loss)\n",
    "        # print(is_correct_loss, correction_loss, is_upper_loss)\n",
    "        # Combine the two losses\n",
    "        total_loss = self.weight[0] * correction_loss + self.weight[1] * is_correct_loss + self.weight[2] * is_upper_loss\n",
    "\n",
    "        return total_loss, {\"correct_loss\":correction_loss.detach().cpu().item(),\"spell_loss\":is_correct_loss.detach().cpu().item(),\"upper_loss\":is_upper_loss.detach().cpu().item()}\n",
    "\n",
    "# Create an instance of your model\n",
    "spell_model = SpellCorrectionModel(13130,214).to(device)\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = optim.Adam(spell_model.parameters(), lr=1.76e-4)\n",
    "\n",
    "# Define your custom loss function\n",
    "loss_function = CustomLoss()\n",
    "bce_l = nn.BCELoss(reduction='sum').to(device)\n",
    "\n",
    "# Define your training loop\n",
    "def train(model, optimizer, loss_function, word_text, char_text, target):\n",
    "    model.train()\n",
    "\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
    "\n",
    "    #print(output_spell)\n",
    "\n",
    "    # print(\"Model output shape : \",output_correction.shape, output_spell.shape)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss, detail = loss_function(output_correction, output_spell, output_upper, target, total_length)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(param.requires_grad)\n",
    "    #     if param.grad is not None:\n",
    "    #         print(f'Parameter: {name}, Gradient Norm: {param.grad.norm().item()}')\n",
    "    #     else:\n",
    "    #         print(f'Parameter: {name}, Gradient: None')\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), detail\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "579a9111-dfe2-4436-8fea-a9f74b9cb39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generate</th>\n",
       "      <th>spell_label</th>\n",
       "      <th>cap_label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chây ì nộp phạt nguội .</td>\n",
       "      <td>Chây ì nộp phạt ngui .</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hàng chục ngàn phương tiện bị ghi hình vi phạm...</td>\n",
       "      <td>Hàng chxục ngàn phương tiện bị ghi hình vo phạ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trên cổng thông tin điện tử của Công an TP . H...</td>\n",
       "      <td>Trên cong thông tin điện tử ca Công ắn YP . HC...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Đây là các phương tiện vi phạm được camera ( d...</td>\n",
       "      <td>Đây lờ các phương tiện vi phạm được camera ( d...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Điều đáng nói , dù Phòng CSGT đường bộ - đường...</td>\n",
       "      <td>Điều đáng nói , dù Pho2ng CSGT đường bộ - đườn...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                            Chây ì nộp phạt nguội .   \n",
       "1  Hàng chục ngàn phương tiện bị ghi hình vi phạm...   \n",
       "2  Trên cổng thông tin điện tử của Công an TP . H...   \n",
       "3  Đây là các phương tiện vi phạm được camera ( d...   \n",
       "4  Điều đáng nói , dù Phòng CSGT đường bộ - đường...   \n",
       "\n",
       "                                            generate  \\\n",
       "0                             Chây ì nộp phạt ngui .   \n",
       "1  Hàng chxục ngàn phương tiện bị ghi hình vo phạ...   \n",
       "2  Trên cong thông tin điện tử ca Công ắn YP . HC...   \n",
       "3  Đây lờ các phương tiện vi phạm được camera ( d...   \n",
       "4  Điều đáng nói , dù Pho2ng CSGT đường bộ - đườn...   \n",
       "\n",
       "                                         spell_label  \\\n",
       "0                                 [0, 0, 0, 0, 1, 0]   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           cap_label  word_count  \n",
       "0                                 [1, 0, 0, 0, 0, 0]           6  \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...          31  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, ...          74  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          23  \n",
       "4  [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...          49  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train_format.csv\")\n",
    "df['word_count'] = df['text'].apply(lambda x : len(x.split(' ')))\n",
    "df = df[(df['text'].apply(len) > 10)]\n",
    "df = df[df['word_count'] <= 500]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6af177c-9f65-4ae1-b54d-b925a6b3f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_to_max_length(x, max_l = 10):\n",
    "    if isinstance(x, str):\n",
    "        x = ast.literal_eval(x)\n",
    "    while len(x) < max_l:\n",
    "        x.append(0)\n",
    "    return x\n",
    "\n",
    "def get_correct(text, generate):\n",
    "    text = text.split(' ')\n",
    "    generate = generate.split(' ')\n",
    "    res = []\n",
    "    for i,j in zip(text, generate):\n",
    "        if i != j:\n",
    "            res.append(map_word[i.lower() if i.lower() in map_word.keys() else \"<unk>\"])\n",
    "        else:\n",
    "            res.append(1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23154e30-9560-483f-8c48-682e46c018da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3581"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_word['chây']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5542d8cf-694d-4106-bb43-732ce204a9a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca27f2b1bb848ce93dcd6aac97ead80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Batch 0 - Avg Loss: 2.6573\n",
      "{'correct_loss': 2.5494680404663086, 'spell_loss': 0.06143359839916229, 'upper_loss': 0.04636283591389656}\n",
      "Epoch 1 - Batch 1000 - Avg Loss: 1.4071\n",
      "{'correct_loss': 1.3033928088247777, 'spell_loss': 0.06891965879499912, 'upper_loss': 0.034803896919474935}\n",
      "Epoch 1 - Batch 2000 - Avg Loss: 1.5773\n",
      "{'correct_loss': 1.4651899276077747, 'spell_loss': 0.08093883001804351, 'upper_loss': 0.031191074563510484}\n",
      "Epoch 1 - Batch 3000 - Avg Loss: 1.5628\n",
      "{'correct_loss': 1.4503035054206848, 'spell_loss': 0.0758976485952735, 'upper_loss': 0.036570466299832335}\n",
      "Epoch 1 - Batch 4000 - Avg Loss: 1.4517\n",
      "{'correct_loss': 1.3393171309828757, 'spell_loss': 0.07679312691837549, 'upper_loss': 0.03563258118071826}\n",
      "Epoch 1 - Batch 5000 - Avg Loss: 1.4339\n",
      "{'correct_loss': 1.3225603160113095, 'spell_loss': 0.07749408227391541, 'upper_loss': 0.033873894604155796}\n",
      "Epoch 1 - Batch 6000 - Avg Loss: 1.5037\n",
      "{'correct_loss': 1.3930859425365925, 'spell_loss': 0.07949477744475007, 'upper_loss': 0.031160261253418867}\n",
      "Epoch 1 - Batch 7000 - Avg Loss: 1.5220\n",
      "{'correct_loss': 1.4107683435380458, 'spell_loss': 0.082733791526407, 'upper_loss': 0.02851122543695965}\n",
      "Epoch 1 - Batch 8000 - Avg Loss: 1.5718\n",
      "{'correct_loss': 1.4566284027397634, 'spell_loss': 0.08345578811131418, 'upper_loss': 0.03167686727296677}\n",
      "torch.Size([16, 112])\n",
      "torch.Size([16, 112])\n",
      "torch.Size([16, 112])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5224, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 4765, in _in_projection_packed\n",
      "    proj = linear(q, w, b)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 43])\n",
      "torch.Size([16, 43])\n",
      "torch.Size([16, 43])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 589, in forward\n",
      "    x = x + self._ff_block(self.norm2(x))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 607, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 62])\n",
      "torch.Size([16, 62])\n",
      "torch.Size([16, 62])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5377, in multi_head_attention_forward\n",
      "    attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 149])\n",
      "torch.Size([16, 149])\n",
      "torch.Size([16, 149])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1217, in forward\n",
      "    return attn_output.transpose(1, 0), attn_output_weights\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 397, in _extract_from_extended_frame_gen\n",
      "    @classmethod\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 81])\n",
      "torch.Size([16, 81])\n",
      "torch.Size([16, 81])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 589, in forward\n",
      "    x = x + self._ff_block(self.norm2(x))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 607, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "                                                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 86])\n",
      "torch.Size([16, 86])\n",
      "torch.Size([16, 86])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 589, in forward\n",
      "    x = x + self._ff_block(self.norm2(x))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 607, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "                                                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 428, in _extract_from_extended_frame_gen\n",
      "    result.append(FrameSummary(\n",
      "                  ^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 264, in __init__\n",
      "    def __init__(self, filename, lineno, name, *, lookup_line=True,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 80])\n",
      "torch.Size([16, 80])\n",
      "torch.Size([16, 80])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5224, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 4765, in _in_projection_packed\n",
      "    proj = linear(q, w, b)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 67])\n",
      "torch.Size([16, 67])\n",
      "torch.Size([16, 67])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1217, in forward\n",
      "    return attn_output.transpose(1, 0), attn_output_weights\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 436, in _extract_from_extended_frame_gen\n",
      "    f.line\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 316, in line\n",
      "    @property\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 65])\n",
      "torch.Size([16, 65])\n",
      "torch.Size([16, 65])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 321, in forward\n",
      "    output = self.norm(output)\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/normalization.py\", line 190, in forward\n",
      "    return F.layer_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 2515, in layer_norm\n",
      "    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 405, in _extract_from_extended_frame_gen\n",
      "    limit = getattr(sys, 'tracebacklimit', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 78])\n",
      "torch.Size([16, 78])\n",
      "torch.Size([16, 78])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5370, in multi_head_attention_forward\n",
      "    k = k.view(bsz, num_heads, src_len, head_dim)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 60])\n",
      "torch.Size([16, 60])\n",
      "torch.Size([16, 60])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5224, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 4767, in _in_projection_packed\n",
      "    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 232, in extract_stack\n",
      "    stack.reverse()\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 67])\n",
      "torch.Size([16, 67])\n",
      "torch.Size([16, 67])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5373, in multi_head_attention_forward\n",
      "    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 436, in _extract_from_extended_frame_gen\n",
      "    f.line\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 321, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 30, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 36, in getlines\n",
      "    def getlines(filename, module_globals=None):\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 76])\n",
      "torch.Size([16, 76])\n",
      "torch.Size([16, 76])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5277, in multi_head_attention_forward\n",
      "    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 421, in _extract_from_extended_frame_gen\n",
      "    fnames.add(filename)\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 46])\n",
      "torch.Size([16, 46])\n",
      "torch.Size([16, 46])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5224, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 4765, in _in_projection_packed\n",
      "    proj = linear(q, w, b)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 41, in format_list\n",
      "    return StackSummary.from_list(extracted_list).format()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 531, in format\n",
      "    formatted_frame = self.format_frame_summary(frame_summary)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 511, in format_frame_summary\n",
      "    return ''.join(row)\n",
      "           ^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 49])\n",
      "torch.Size([16, 49])\n",
      "torch.Size([16, 49])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5374, in multi_head_attention_forward\n",
      "    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "                           ^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/normalization.py\", line 190, in forward\n",
      "    return F.layer_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 2515, in layer_norm\n",
      "    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 41, in format_list\n",
      "    return StackSummary.from_list(extracted_list).format()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 531, in format\n",
      "    formatted_frame = self.format_frame_summary(frame_summary)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 468, in format_frame_summary\n",
      "    stripped_line = frame_summary.line.strip()\n",
      "                    ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 316, in line\n",
      "    @property\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 72])\n",
      "torch.Size([16, 72])\n",
      "torch.Size([16, 72])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 589, in forward\n",
      "    x = x + self._ff_block(self.norm2(x))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 607, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 421, in _extract_from_extended_frame_gen\n",
      "    fnames.add(filename)\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 75])\n",
      "torch.Size([16, 75])\n",
      "torch.Size([16, 75])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 589, in forward\n",
      "    x = x + self._ff_block(self.norm2(x))\n",
      "                           ^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1601, in __getattr__\n",
      "    def __getattr__(self, name: str) -> Union[Tensor, 'Module']:\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 124])\n",
      "torch.Size([16, 124])\n",
      "torch.Size([16, 124])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "        ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 57])\n",
      "torch.Size([16, 57])\n",
      "torch.Size([16, 57])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "                           ^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/normalization.py\", line 190, in forward\n",
      "    return F.layer_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 2515, in layer_norm\n",
      "    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 29, in format_list\n",
      "    def format_list(extracted_list):\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 48])\n",
      "torch.Size([16, 48])\n",
      "torch.Size([16, 48])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5277, in multi_head_attention_forward\n",
      "    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 111])\n",
      "torch.Size([16, 111])\n",
      "torch.Size([16, 111])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 589, in forward\n",
      "    x = x + self._ff_block(self.norm2(x))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 607, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 421, in _extract_from_extended_frame_gen\n",
      "    fnames.add(filename)\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 45])\n",
      "torch.Size([16, 45])\n",
      "torch.Size([16, 45])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5224, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 4768, in _in_projection_packed\n",
      "    return proj[0], proj[1], proj[2]\n",
      "                    ~~~~^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 46])\n",
      "torch.Size([16, 46])\n",
      "torch.Size([16, 46])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 603, in _sa_block\n",
      "    return self.dropout1(x)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 1252, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 43])\n",
      "torch.Size([16, 43])\n",
      "torch.Size([16, 43])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 589, in forward\n",
      "    x = x + self._ff_block(self.norm2(x))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 607, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "                                                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 59])\n",
      "torch.Size([16, 59])\n",
      "torch.Size([16, 59])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 589, in forward\n",
      "    x = x + self._ff_block(self.norm2(x))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 607, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 35, in format_stack\n",
      "    @compatibility(is_backward_compatible=False)\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 76])\n",
      "torch.Size([16, 76])\n",
      "torch.Size([16, 76])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5224, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 4767, in _in_projection_packed\n",
      "    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 421, in _extract_from_extended_frame_gen\n",
      "    fnames.add(filename)\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 38])\n",
      "torch.Size([16, 38])\n",
      "torch.Size([16, 38])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 89, in forward\n",
      "    src = self.pos_encoder(linear_input)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 35, in forward\n",
      "    x = x + self.pe[:x.size(0)]\n",
      "        ~~^~~~~~~~~~~~~~~~~~~~~\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 43])\n",
      "torch.Size([16, 43])\n",
      "torch.Size([16, 43])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5371, in multi_head_attention_forward\n",
      "    v = v.view(bsz, num_heads, src_len, head_dim)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 421, in _extract_from_extended_frame_gen\n",
      "    fnames.add(filename)\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 81])\n",
      "torch.Size([16, 81])\n",
      "torch.Size([16, 81])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "                           ^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/normalization.py\", line 190, in forward\n",
      "    return F.layer_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 2515, in layer_norm\n",
      "    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 428, in _extract_from_extended_frame_gen\n",
      "    result.append(FrameSummary(\n",
      "                  ^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 264, in __init__\n",
      "    def __init__(self, filename, lineno, name, *, lookup_line=True,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 102])\n",
      "torch.Size([16, 102])\n",
      "torch.Size([16, 102])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 589, in forward\n",
      "    x = x + self._ff_block(self.norm2(x))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 607, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 49])\n",
      "torch.Size([16, 49])\n",
      "torch.Size([16, 49])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5224, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 4767, in _in_projection_packed\n",
      "    proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 436, in _extract_from_extended_frame_gen\n",
      "    f.line\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 322, in line\n",
      "    return self._line.strip()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 66])\n",
      "torch.Size([16, 66])\n",
      "torch.Size([16, 66])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "        ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 421, in _extract_from_extended_frame_gen\n",
      "    fnames.add(filename)\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 49])\n",
      "torch.Size([16, 49])\n",
      "torch.Size([16, 49])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5224, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 4765, in _in_projection_packed\n",
      "    proj = linear(q, w, b)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 421, in _extract_from_extended_frame_gen\n",
      "    fnames.add(filename)\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 57])\n",
      "torch.Size([16, 57])\n",
      "torch.Size([16, 57])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 87, in forward\n",
      "    char_embedded_text = self.char_transformer_encoder(char_embedded_text, src_key_padding_mask=padding_mask_w)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 588, in forward\n",
      "    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 5279, in multi_head_attention_forward\n",
      "    k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 174])\n",
      "torch.Size([16, 174])\n",
      "torch.Size([16, 174])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 90, in forward\n",
      "    output = self.word_transformer_encoder(src, src_key_padding_mask=padding_mask_w)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 315, in forward\n",
      "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 589, in forward\n",
      "    x = x + self._ff_block(self.norm2(x))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 607, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "torch.Size([16, 79])\n",
      "torch.Size([16, 79])\n",
      "torch.Size([16, 79])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_312225/538821458.py\", line 35, in <module>\n",
      "    loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_312225/2923624309.py\", line 83, in train\n",
      "    output_correction, output_spell, output_upper, total_length = model(word_text, char_text)\n",
      "                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/spell/model.py\", line 96, in forward\n",
      "    output_spell = self.is_correct(output_spell)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/fx/traceback.py\", line 41, in format_stack\n",
      "    return traceback.format_list(traceback.extract_stack()[:-1])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 231, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 393, in extract\n",
      "    return klass._extract_from_extended_frame_gen(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/traceback.py\", line 432, in _extract_from_extended_frame_gen\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.11/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"EPOCH : {}\".format(epoch))\n",
    "    t_loss = []\n",
    "    batch_detail = {\"correct_loss\":[],\"spell_loss\":[],\"upper_loss\":[]}\n",
    "    for i in tqdm(range(0, df.shape[0]//batch_size, 1)):\n",
    "        # Get the current batch\n",
    "        tmp_data = df.iloc[batch_size * i: batch_size * (i+1)].to_dict('records')\n",
    "        # print(tmp_data)\n",
    "\n",
    "        batch_input_gt = [x['text'] for x in tmp_data]\n",
    "        batch_input = [x['generate'] for x in tmp_data]\n",
    "\n",
    "        # print(batch_input, batch_input_gt)\n",
    "\n",
    "        t, c, max_batch_len = mapping_batch(batch_input)\n",
    "        if max_batch_len == 0:\n",
    "            continue\n",
    "        # print(max_batch_len)\n",
    "        label_spell = [expand_to_max_length(x['spell_label'], max_batch_len) for x in tmp_data]\n",
    "        label_cap = [expand_to_max_length(x['cap_label'], max_batch_len) for x in tmp_data]\n",
    "        label_correction = [get_correct(x['text'], x['generate']) for x in tmp_data]\n",
    "        label_correction = [expand_to_max_length(x, max_batch_len) for x in label_correction]\n",
    "        target = {\n",
    "            \"spell\" : torch.tensor(label_spell),\n",
    "            \"upper\" : torch.tensor(label_cap),\n",
    "            \"correction\" : torch.tensor(label_correction)\n",
    "        }\n",
    "        \n",
    "        # break\n",
    "        try:\n",
    "            # if t_correction.shape != torch.tensor(t).shape:\n",
    "            #     continue\n",
    "            loss, detail = train(spell_model, optimizer, loss_function, t, c, target)\n",
    "            t_loss.append(loss)\n",
    "            for detail_k in detail.keys():\n",
    "                batch_detail[detail_k].append(detail[detail_k])\n",
    "                \n",
    "        except:\n",
    "            # continue\n",
    "            print(torch.tensor(label_spell).shape)\n",
    "            print(torch.tensor(label_cap).shape)\n",
    "            print(torch.tensor(label_correction).shape)\n",
    "            print(traceback.format_exc())\n",
    "            # print(batch_train_label.shape, t_correction.shape)\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Epoch {epoch+1} - Batch {i} - Avg Loss: {(sum(t_loss)/len(t_loss)):.4f}\")\n",
    "            detail = {bdt: sum(batch_detail[bdt])/(len(batch_detail[bdt])) for bdt in batch_detail.keys()}\n",
    "            print(detail)\n",
    "            batch_detail = {\"correct_loss\":[],\"spell_loss\":[],\"upper_loss\":[]}\n",
    "            t_loss = []\n",
    "            if i % 2000 == 0 and i > 1:\n",
    "                torch.save({\n",
    "                    'model_state_dict': spell_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, f'model_epochs_{epoch+1}_batch_{i}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ae2e622-66ca-49c1-b070-66ed38850011",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': spell_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, f'model_latest.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8e230c-d1e8-4985-91ee-0333f8e494bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "bce_l = nn.BCELoss(reduction='sum')\n",
    "bce_l(torch.tensor([0.0,0.0]),torch.tensor([0.0,0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3550e1aa-334e-4de0-8b58-07ffb79f8a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           Trong dài hạn , Siegel cho biết : Những mối qu...\n",
       "generate       Trong dài hạn , Siegel cho biết : Như4ng mối q...\n",
       "spell_label    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...\n",
       "cap_label      [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
       "word_count                                                    21\n",
       "Name: 999999, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93366ea2-d804-4daf-987e-94454918bc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-1]['spell_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4be9f70-4c8c-4b0d-a1da-54c436bdd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, c, max_length = mapping_batch([df['generate'].iloc[0],df['generate'].iloc[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f587847-17b8-49f0-8e46-76b9d86f5c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3581, 3364, 965, 729, 3, 13112, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [11,\n",
       "  418,\n",
       "  461,\n",
       "  13110,\n",
       "  3,\n",
       "  12,\n",
       "  80,\n",
       "  13114,\n",
       "  3,\n",
       "  812,\n",
       "  59,\n",
       "  261,\n",
       "  24,\n",
       "  13,\n",
       "  3,\n",
       "  60,\n",
       "  3,\n",
       "  29,\n",
       "  552,\n",
       "  1201,\n",
       "  13112]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a14736f1-32f5-4e5c-9d75-56e1a7cc4b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n"
     ]
    }
   ],
   "source": [
    "spell_model.eval()\n",
    "with torch.no_grad():\n",
    "    a, b, c, d = spell_model(w,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1008223c-9e69-4e7c-bbfc-f5d166568622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff5ff9e0-bd81-4a15-8bc5-8f4d912b0678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5017, 0.5006, 0.5002, 0.5006, 0.6974, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000],\n",
       "        [0.5002, 0.5045, 0.5026, 0.5000, 0.5027, 0.5009, 0.5004, 0.5003, 0.7283,\n",
       "         0.5020, 0.5010, 0.5003, 0.5011, 0.5001, 0.6719, 0.5002, 0.7303, 0.5025,\n",
       "         0.5030, 0.5023, 0.5000]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5776b8b-e606-4138-855b-cc622efe1f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7310, 0.5006, 0.5000, 0.5000, 0.5000, 0.5001, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000],\n",
       "        [0.7311, 0.5000, 0.5000, 0.5000, 0.7299, 0.5000, 0.5000, 0.5000, 0.7310,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5001, 0.5002, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9dd9d5fb-4976-4253-9d90-ef78a68c8b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Laurence Siegel , từng pgụ trách mảng nghiên cứu tại F ord F oundation , cho rằng sự thăng trầm của quỹ thuộc Mindich cho thấy Harvard cần giữ khoảng cách hơn với những nhà quản lý có mối quan hệ với Trường Đại học .'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-2]['generate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75b9ff39-94a2-4d2f-bb16-0c99efe1bec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Laurence Siegel , từng phụ trách mảng nghiên cứu tại F ord F oundation , cho rằng sự thăng trầm của quỹ thuộc Mindich cho thấy Harvard cần giữ khoảng cách hơn với những nhà quản lý có mối quan hệ với Trường Đại học .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5eac1a2d-ad5c-4f79-b600-df0c0ca128bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 332,  322,  965,  648,  589, 1599,   26,   26,    9,    9,    9,    9,\n",
       "            9,   26,   26,   26,    9,    9,    9,   26,   26],\n",
       "        [  11,  418,  114, 1993, 2336,   12,   80, 1965,   74,  812,  464,  261,\n",
       "           48, 2063,   90,   60,  229,   29,  552, 1201,  111]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4095828c-a800-43af-9608-ddf04ada34b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5017, 0.5006, 0.5002, 0.5006, 0.6974, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000],\n",
       "        [0.5002, 0.5045, 0.5026, 0.5000, 0.5027, 0.5009, 0.5004, 0.5003, 0.7283,\n",
       "         0.5020, 0.5010, 0.5003, 0.5011, 0.5001, 0.6719, 0.5002, 0.7303, 0.5025,\n",
       "         0.5030, 0.5023, 0.5000]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f47076c7-09ae-48cd-9be8-bc79c0bf5c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thực'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_map_word[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1ffc7d7-1ac1-49bd-b71d-8bb4a063beea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "spell_model = SpellCorrectionModel(13130,214).to(device)\n",
    "spell_model.load_state_dict(torch.load(\"model_loss_1.8290443115711212.pth\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e841da4-6280-49bf-a2a2-3d7ffd0fc180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :  Mô5t vai vị khách còn không kìm được nước mắt .\n",
      "Groundtruth :  Một vài vị khách còn không kìm được nước mắt .\n",
      "Fix :  một vai vị khách còn không kìm được nước mắt .\n"
     ]
    }
   ],
   "source": [
    "g = -196\n",
    "input_sen = df['generate'].iloc[g]\n",
    "label = df['text'].iloc[g]\n",
    "w, c, max_length = mapping_batch([\"tét\",input_sen])\n",
    "spell_model.eval()\n",
    "with torch.no_grad():\n",
    "    a, b, c, d = spell_model(w,c)\n",
    "correction = torch.argmax(a, dim=-1).tolist()[1]\n",
    "spell_output = torch.sigmoid(b).tolist()[1]\n",
    "spell_output = [1 if i > 0.7 else 0 for i in spell_output ]\n",
    "upper_output = torch.sigmoid(b).tolist()[1]\n",
    "upper_output = [1 if i > 0.7 else 0 for i in upper_output ]\n",
    "res = []\n",
    "for i, j, k in zip(input_sen.split(' '), spell_output, correction):\n",
    "    if j == 1:\n",
    "        res.append(reverse_map_word[k])\n",
    "    else:\n",
    "        res.append(i)\n",
    "print(\"Input : \",input_sen)\n",
    "print(\"Groundtruth : \",label)\n",
    "print(\"Fix : \",' '.join(x for x in res))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df871070-3390-4380-bd48-e429d85e68cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text            Một vài vị khách còn không kìm được nước mắt .\n",
       "generate       Mô5t vai vị khách còn không kìm được nước mắt .\n",
       "spell_label                  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "cap_label                    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "word_count                                                  11\n",
       "Name: 999802, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "058a3311-f93b-4eb4-b7f5-d8f9bee50365",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = [df.iloc[g]]\n",
    "batch_input_gt = [x['text'] for x in tmp_data]\n",
    "batch_input_gt\n",
    "label_correction = [get_correct(x['text'], x['generate']) for x in tmp_data]\n",
    "label_correction = [expand_to_max_length(x, 11) for x in label_correction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a68b8da-8d8b-4653-83aa-a141f0a52a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16, 663, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a93919b3-23c0-48b4-9e40-de80902a16d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_map_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08ed34dd-3e10-4e23-86e3-f2e841fbfac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3997e+00,  1.9536e+00, -2.3226e-01, -1.3761e+00,  3.3113e-02,\n",
       "          -5.7060e-01, -1.7402e-01,  5.9783e-01, -1.1561e+00, -4.7511e-01],\n",
       "         [ 1.0332e+00,  1.7896e+00, -5.2333e-01, -1.1759e+00, -2.9186e-01,\n",
       "           2.2583e-03,  1.0198e-01,  1.2120e+00, -7.5648e-01, -1.3915e+00]],\n",
       "\n",
       "        [[-2.4668e-01,  1.6173e+00, -9.8781e-01, -9.7483e-01,  3.7100e-01,\n",
       "           3.5459e-01,  5.2244e-01,  1.4033e+00, -4.0863e-01, -1.6507e+00],\n",
       "         [ 1.1081e+00,  1.5793e+00, -6.1418e-03, -1.4710e+00, -7.1274e-01,\n",
       "          -9.2333e-01, -1.9583e-01,  1.5204e+00, -4.5299e-01, -4.4565e-01]],\n",
       "\n",
       "        [[ 1.8982e-01,  1.5839e+00, -3.6538e-01, -1.6380e+00,  1.2425e+00,\n",
       "          -6.6418e-01, -1.0880e+00,  9.9658e-01, -5.9971e-01,  3.4244e-01],\n",
       "         [-1.2084e+00, -4.6442e-01,  2.5157e-01, -5.5118e-01,  1.4654e+00,\n",
       "           5.9568e-02, -1.9100e+00,  4.3064e-01,  7.2227e-01,  1.2045e+00]],\n",
       "\n",
       "        [[ 4.9238e-01, -7.4072e-01,  3.0203e-01, -1.6311e+00, -1.2081e+00,\n",
       "           4.2787e-01,  4.8466e-01,  1.9939e+00, -6.0665e-01,  4.8576e-01],\n",
       "         [ 2.9838e-01,  1.9694e+00, -7.2931e-01, -8.2401e-01, -2.4698e-01,\n",
       "          -3.9547e-01,  1.2995e-01,  1.6777e+00, -8.8361e-01, -9.9599e-01]],\n",
       "\n",
       "        [[ 1.0757e+00,  2.2495e+00, -1.0998e+00, -5.6703e-01,  6.5589e-01,\n",
       "          -8.6672e-01, -6.7957e-01, -8.3908e-02,  8.6472e-02, -7.7052e-01],\n",
       "         [ 1.0345e+00,  2.4227e+00, -1.7062e-01, -1.2227e+00, -8.9949e-01,\n",
       "           3.5857e-02, -5.6920e-01,  2.1207e-01, -3.9635e-01, -4.4684e-01]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8398c0c-e52e-42e7-a039-6792f33d5a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_171466/1088423724.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
      "  out.grad\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7cd52-8d32-4c10-b561-8d6e1f72f841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
